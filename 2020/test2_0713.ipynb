{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3.6.8/envs/combo3 (set by /Users/mkumada/Documents/class/2020/work/combo3/.python-version)\n"
    }
   ],
   "source": [
    "!pyenv version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['/Users/mkumada/Documents/class/2020/work/combo3/mywork',\n '/Users/mkumada/.vscode/extensions/ms-python.python-2020.8.108011/pythonFiles',\n '/Users/mkumada/.vscode/extensions/ms-python.python-2020.8.108011/pythonFiles/lib/python',\n '/Users/mkumada/.pyenv/versions/3.6.8/lib/python36.zip',\n '/Users/mkumada/.pyenv/versions/3.6.8/lib/python3.6',\n '/Users/mkumada/.pyenv/versions/3.6.8/lib/python3.6/lib-dynload',\n '',\n '/Users/mkumada/.pyenv/versions/combo3/lib/python3.6/site-packages',\n '/Users/mkumada/.pyenv/versions/combo3/lib/python3.6/site-packages/IPython/extensions',\n '/Users/mkumada/.ipython',\n '/Users/mkumada/.pyenv/versions/3.6.8/envs/combo3/lib/python3.6/site-packages/']\n"
    }
   ],
   "source": [
    "import sys\n",
    "import pprint\n",
    "sys.path.append('/Users/mkumada/.pyenv/versions/3.6.8/envs/combo3/lib/python3.6/site-packages/')\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "# import cPickle as pickle\n",
    "import _pickle as cPickle\n",
    "import scipy\n",
    "import combo\n",
    "import os\n",
    "import urllib\n",
    "import time\n",
    "from combo.variable import variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # current_dir = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
    "    current_dir = './'\n",
    "    arr = np.genfromtxt(os.path.join(current_dir, 'data.csv'), skip_header=1, delimiter=',')\n",
    "    # 対象列に値が埋められているサンプルだけを学習データとして抽出します。\n",
    "\n",
    "    arr_train = arr[~np.isnan(arr[:, 0]), :]\n",
    "    arr_test = arr[np.isnan(arr[:, 0]), :]\n",
    "    X_train = arr_train[:, 1:]\n",
    "    # 既に検討した候補\n",
    "    t_train = arr_train[:, 0]\n",
    "\n",
    "\n",
    "    # 対象列が空であるサンプルをテストデータとして抽出します。\n",
    "    X_test = arr_test[:, 1:]\n",
    "    # 次に検討すべき候補\n",
    "    # テストデータのインデックスのリストを取得します。\n",
    "    # 元となった csv ファイルの行番号に合わせるため、+2 しておきます。\n",
    "    test_idx_list = np.where(np.isnan(arr[:, 0]))[0].tolist()\n",
    "    test_idx_list = [i + 2 for i in test_idx_list]\n",
    "\n",
    "    X_all=arr[:, 1:]\n",
    "\n",
    "    return X_train, t_train, X_test, test_idx_list, X_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.56       0.13856406]\n [0.7        0.17320508]\n [0.46       0.31176915]\n [0.76       0.34641016]\n [0.5        0.45033321]]\n[[0.         0.        ]\n [0.04       0.        ]\n [0.08       0.        ]\n [0.12       0.        ]\n [0.16       0.        ]\n [0.2        0.        ]\n [0.24       0.        ]\n [0.28       0.        ]\n [0.32       0.        ]\n [0.36       0.        ]\n [0.4        0.        ]\n [0.44       0.        ]\n [0.48       0.        ]\n [0.52       0.        ]\n [0.56       0.        ]\n [0.6        0.        ]\n [0.64       0.        ]\n [0.68       0.        ]\n [0.72       0.        ]\n [0.76       0.        ]\n [0.8        0.        ]\n [0.84       0.        ]\n [0.88       0.        ]\n [0.92       0.        ]\n [0.96       0.        ]\n [1.         0.        ]\n [0.02       0.03464102]\n [0.06       0.03464102]\n [0.1        0.03464102]\n [0.14       0.03464102]\n [0.18       0.03464102]\n [0.22       0.03464102]\n [0.26       0.03464102]\n [0.3        0.03464102]\n [0.34       0.03464102]\n [0.38       0.03464102]\n [0.42       0.03464102]\n [0.46       0.03464102]\n [0.5        0.03464102]\n [0.54       0.03464102]\n [0.58       0.03464102]\n [0.62       0.03464102]\n [0.66       0.03464102]\n [0.7        0.03464102]\n [0.74       0.03464102]\n [0.78       0.03464102]\n [0.82       0.03464102]\n [0.86       0.03464102]\n [0.9        0.03464102]\n [0.94       0.03464102]\n [0.98       0.03464102]\n [0.04       0.06928203]\n [0.08       0.06928203]\n [0.12       0.06928203]\n [0.16       0.06928203]\n [0.2        0.06928203]\n [0.24       0.06928203]\n [0.28       0.06928203]\n [0.32       0.06928203]\n [0.36       0.06928203]\n [0.4        0.06928203]\n [0.44       0.06928203]\n [0.48       0.06928203]\n [0.52       0.06928203]\n [0.56       0.06928203]\n [0.6        0.06928203]\n [0.64       0.06928203]\n [0.68       0.06928203]\n [0.72       0.06928203]\n [0.76       0.06928203]\n [0.8        0.06928203]\n [0.84       0.06928203]\n [0.88       0.06928203]\n [0.92       0.06928203]\n [0.96       0.06928203]\n [0.06       0.10392305]\n [0.1        0.10392305]\n [0.14       0.10392305]\n [0.18       0.10392305]\n [0.22       0.10392305]\n [0.26       0.10392305]\n [0.3        0.10392305]\n [0.34       0.10392305]\n [0.38       0.10392305]\n [0.42       0.10392305]\n [0.46       0.10392305]\n [0.5        0.10392305]\n [0.54       0.10392305]\n [0.58       0.10392305]\n [0.62       0.10392305]\n [0.66       0.10392305]\n [0.7        0.10392305]\n [0.74       0.10392305]\n [0.78       0.10392305]\n [0.82       0.10392305]\n [0.86       0.10392305]\n [0.9        0.10392305]\n [0.94       0.10392305]\n [0.08       0.13856406]\n [0.12       0.13856406]\n [0.16       0.13856406]\n [0.2        0.13856406]\n [0.24       0.13856406]\n [0.28       0.13856406]\n [0.32       0.13856406]\n [0.36       0.13856406]\n [0.4        0.13856406]\n [0.44       0.13856406]\n [0.48       0.13856406]\n [0.52       0.13856406]\n [0.6        0.13856406]\n [0.64       0.13856406]\n [0.68       0.13856406]\n [0.72       0.13856406]\n [0.76       0.13856406]\n [0.8        0.13856406]\n [0.84       0.13856406]\n [0.88       0.13856406]\n [0.92       0.13856406]\n [0.1        0.17320508]\n [0.14       0.17320508]\n [0.18       0.17320508]\n [0.22       0.17320508]\n [0.26       0.17320508]\n [0.3        0.17320508]\n [0.34       0.17320508]\n [0.38       0.17320508]\n [0.42       0.17320508]\n [0.46       0.17320508]\n [0.5        0.17320508]\n [0.54       0.17320508]\n [0.58       0.17320508]\n [0.62       0.17320508]\n [0.66       0.17320508]\n [0.74       0.17320508]\n [0.78       0.17320508]\n [0.82       0.17320508]\n [0.86       0.17320508]\n [0.9        0.17320508]\n [0.12       0.2078461 ]\n [0.16       0.2078461 ]\n [0.2        0.2078461 ]\n [0.24       0.2078461 ]\n [0.28       0.2078461 ]\n [0.32       0.2078461 ]\n [0.36       0.2078461 ]\n [0.4        0.2078461 ]\n [0.44       0.2078461 ]\n [0.48       0.2078461 ]\n [0.52       0.2078461 ]\n [0.56       0.2078461 ]\n [0.6        0.2078461 ]\n [0.64       0.2078461 ]\n [0.68       0.2078461 ]\n [0.72       0.2078461 ]\n [0.76       0.2078461 ]\n [0.8        0.2078461 ]\n [0.84       0.2078461 ]\n [0.88       0.2078461 ]\n [0.14       0.24248711]\n [0.18       0.24248711]\n [0.22       0.24248711]\n [0.26       0.24248711]\n [0.3        0.24248711]\n [0.34       0.24248711]\n [0.38       0.24248711]\n [0.42       0.24248711]\n [0.46       0.24248711]\n [0.5        0.24248711]\n [0.54       0.24248711]\n [0.58       0.24248711]\n [0.62       0.24248711]\n [0.66       0.24248711]\n [0.7        0.24248711]\n [0.74       0.24248711]\n [0.78       0.24248711]\n [0.82       0.24248711]\n [0.86       0.24248711]\n [0.16       0.27712813]\n [0.2        0.27712813]\n [0.24       0.27712813]\n [0.28       0.27712813]\n [0.32       0.27712813]\n [0.36       0.27712813]\n [0.4        0.27712813]\n [0.44       0.27712813]\n [0.48       0.27712813]\n [0.52       0.27712813]\n [0.56       0.27712813]\n [0.6        0.27712813]\n [0.64       0.27712813]\n [0.68       0.27712813]\n [0.72       0.27712813]\n [0.76       0.27712813]\n [0.8        0.27712813]\n [0.84       0.27712813]\n [0.18       0.31176915]\n [0.22       0.31176915]\n [0.26       0.31176915]\n [0.3        0.31176915]\n [0.34       0.31176915]\n [0.38       0.31176915]\n [0.42       0.31176915]\n [0.5        0.31176915]\n [0.54       0.31176915]\n [0.58       0.31176915]\n [0.62       0.31176915]\n [0.66       0.31176915]\n [0.7        0.31176915]\n [0.74       0.31176915]\n [0.78       0.31176915]\n [0.82       0.31176915]\n [0.2        0.34641016]\n [0.24       0.34641016]\n [0.28       0.34641016]\n [0.32       0.34641016]\n [0.36       0.34641016]\n [0.4        0.34641016]\n [0.44       0.34641016]\n [0.48       0.34641016]\n [0.52       0.34641016]\n [0.56       0.34641016]\n [0.6        0.34641016]\n [0.64       0.34641016]\n [0.68       0.34641016]\n [0.72       0.34641016]\n [0.8        0.34641016]\n [0.22       0.38105118]\n [0.26       0.38105118]\n [0.3        0.38105118]\n [0.34       0.38105118]\n [0.38       0.38105118]\n [0.42       0.38105118]\n [0.46       0.38105118]\n [0.5        0.38105118]\n [0.54       0.38105118]\n [0.58       0.38105118]\n [0.62       0.38105118]\n [0.66       0.38105118]\n [0.7        0.38105118]\n [0.74       0.38105118]\n [0.78       0.38105118]\n [0.24       0.41569219]\n [0.28       0.41569219]\n [0.32       0.41569219]\n [0.36       0.41569219]\n [0.4        0.41569219]\n [0.44       0.41569219]\n [0.48       0.41569219]\n [0.52       0.41569219]\n [0.56       0.41569219]\n [0.6        0.41569219]\n [0.64       0.41569219]\n [0.68       0.41569219]\n [0.72       0.41569219]\n [0.76       0.41569219]\n [0.26       0.45033321]\n [0.3        0.45033321]\n [0.34       0.45033321]\n [0.38       0.45033321]\n [0.42       0.45033321]\n [0.46       0.45033321]\n [0.54       0.45033321]\n [0.58       0.45033321]\n [0.62       0.45033321]\n [0.66       0.45033321]\n [0.7        0.45033321]\n [0.74       0.45033321]\n [0.28       0.48497423]\n [0.32       0.48497423]\n [0.36       0.48497423]\n [0.4        0.48497423]\n [0.44       0.48497423]\n [0.48       0.48497423]\n [0.52       0.48497423]\n [0.56       0.48497423]\n [0.6        0.48497423]\n [0.64       0.48497423]\n [0.68       0.48497423]\n [0.72       0.48497423]\n [0.3        0.51961524]\n [0.34       0.51961524]\n [0.38       0.51961524]\n [0.42       0.51961524]\n [0.46       0.51961524]\n [0.5        0.51961524]\n [0.54       0.51961524]\n [0.58       0.51961524]\n [0.62       0.51961524]\n [0.66       0.51961524]\n [0.7        0.51961524]\n [0.32       0.55425626]\n [0.36       0.55425626]\n [0.4        0.55425626]\n [0.44       0.55425626]\n [0.48       0.55425626]\n [0.52       0.55425626]\n [0.56       0.55425626]\n [0.6        0.55425626]\n [0.64       0.55425626]\n [0.68       0.55425626]\n [0.34       0.58889727]\n [0.38       0.58889727]\n [0.42       0.58889727]\n [0.46       0.58889727]\n [0.5        0.58889727]\n [0.54       0.58889727]\n [0.58       0.58889727]\n [0.62       0.58889727]\n [0.66       0.58889727]\n [0.36       0.62353829]\n [0.4        0.62353829]\n [0.44       0.62353829]\n [0.48       0.62353829]\n [0.52       0.62353829]\n [0.56       0.62353829]\n [0.6        0.62353829]\n [0.64       0.62353829]\n [0.38       0.65817931]\n [0.42       0.65817931]\n [0.46       0.65817931]\n [0.5        0.65817931]\n [0.54       0.65817931]\n [0.58       0.65817931]\n [0.62       0.65817931]\n [0.4        0.69282032]\n [0.44       0.69282032]\n [0.48       0.69282032]\n [0.52       0.69282032]\n [0.56       0.69282032]\n [0.6        0.69282032]\n [0.42       0.72746134]\n [0.46       0.72746134]\n [0.5        0.72746134]\n [0.54       0.72746134]\n [0.58       0.72746134]\n [0.44       0.76210236]\n [0.48       0.76210236]\n [0.52       0.76210236]\n [0.56       0.76210236]\n [0.46       0.79674337]\n [0.5        0.79674337]\n [0.54       0.79674337]\n [0.48       0.83138439]\n [0.52       0.83138439]\n [0.5        0.8660254 ]]\n"
    }
   ],
   "source": [
    "X_train, t_train, X_test, test_idx_list, X_all = load_data()\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training: <combo.variable.variable object at 0x11a57bf98>\nFitting starts\nStart the initial hyper parameter searching ...\nDone\n\nStart the hyper parameter learning ...\n0-th epoch, marginal likelihood 29.203724958978132\n50-th epoch, marginal likelihood 29.17980475930468\n100-th epoch, marginal likelihood 29.1685270712508\n150-th epoch, marginal likelihood 29.163160761847283\n200-th epoch, marginal likelihood 29.160817878344876\n250-th epoch, marginal likelihood 29.159790550670447\n300-th epoch, marginal likelihood 29.159264891444334\n350-th epoch, marginal likelihood 29.158913736700896\n400-th epoch, marginal likelihood 29.158623817771534\n450-th epoch, marginal likelihood 29.15836014197323\n500-th epoch, marginal likelihood 29.158112069236395\nDone\n\n Parameters of Gaussian kernel \n \n width  =  [3.]\n scale  =  1.0\n scale2 =  1.0\n \n\nFitting ends\nCalculating score for test data\nNext Point: [0.74       0.45033321]   Row Number: 274\n"
    }
   ],
   "source": [
    "# Design of policy\n",
    "if __name__==\"__main__\":\n",
    "    # Load the data.\n",
    "    # X is the N x d dimensional matrix. Each row of X denotes the d-dimensional feature vector of search candidate.\n",
    "    # t is the N-dimensional vector that represents the corresponding negative energy of search candidates.\n",
    "    # ( It is of course unknown in practice. )\n",
    "\n",
    "    X_train, t_train, X_test, test_idx_list, X_all = load_data()\n",
    "\n",
    "    # Normalize the mean and standard deviation along the each column of X to 0 and 1, respectively\n",
    "    #X = combo.misc.centering(X)\n",
    "\n",
    "    # feature vector の各要素を正規化します。\n",
    "    ave = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    X_train = (X_train - ave) / (std + 1e-8)\n",
    "    X_test = (X_test - ave) / (std + 1e-8)\n",
    "\n",
    "    train_data = variable(X=X_train, t=t_train)\n",
    "    test_data = variable(X=X_test)\n",
    "\n",
    "    print('training:', train_data)\n",
    "\n",
    "    # Declare the class for calling the simulator.\n",
    "    # In this tutorial, we simply refer to the value of t.\n",
    "    # If you want to apply combo to other problems, you have to customize this class.\n",
    "    # Declaring the policy by\n",
    "    policy = combo.search.discrete.policy(test_X=test_data)\n",
    "    # test_X is the set of candidates which is represented by numpy.array.\n",
    "    # Each row vector represents the feature vector of the corresponding candidate\n",
    "\n",
    "    # set the seed parameter\n",
    "    policy.set_seed(0)\n",
    "\n",
    "    # If you want to perform the initial random search before starting the Bayesian optimization,\n",
    "    # the random sampling is performed by\n",
    "\n",
    "    # 最初のランダムサーチは行わない想定なのでコメントアウトします。\n",
    "    #res = policy.random_search(max_num_probes=20, simulator=simulator())\n",
    "    # Input:\n",
    "    # max_num_probes: number of random search\n",
    "    # simulator = simulator\n",
    "    # output: combo.search.discreate.results (class)\n",
    "\n",
    "    # single query Bayesian search\n",
    "    # The single query version of COMBO is performed by\n",
    "\n",
    "    # テストデータ全体の中から次に評価すべき一点だけを提示するメソッドに差し替えて使用します。\n",
    "    #res = policy.bayes_search(max_num_probes=1, simulator=simulator(), score='TS',\n",
    "    #                          interval=20, num_rand_basis=5000)\n",
    "\n",
    "    action = policy.bayes_search_single(training=train_data, score='TS', num_rand_basis=5000)\n",
    "\n",
    "    # Input\n",
    "    # max_num_probes: number of searching by Bayesian optimization\n",
    "    # simulator: the class of simulator which is defined above\n",
    "    # score: the type of aquision funciton. TS, EI and PI are available\n",
    "    # interval: the timing for learning the hyper parameter.\n",
    "    # In this case, the hyper parameter is learned at each 20 steps\n",
    "    # If you set the negative value to interval, the hyper parameter learning is not performed\n",
    "    # If you set zero to interval, the hyper parameter learning is performed only at the first step\n",
    "    # num_rand_basis: the number of basis function. If you choose 0,  ordinary Gaussian process runs\n",
    "\n",
    "    # save the results\n",
    "    #res.save('test.npz')\n",
    "    # テストデータの行番号リストの中からaction番目のものを提示する\n",
    "\n",
    "\n",
    "    print(\"Next Point: \" + str(X_all[test_idx_list[action]-2]) + \"   Row Number: \" + str(test_idx_list[action]))\n",
    "\n",
    "# sys.exit()\n",
    "# time.sleep(1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('combo3': venv)",
   "language": "python",
   "name": "python36864bitcombo3venv3238eef32288454ebc1f541b15324ee6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}